{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从0手撸FlashAttention\n",
    "要注意的是 上面的PyTorch 实现并没有用到 Shared Memory，它只是演示了 FlashAttention 的思想流程。真正利用了 SRAM 的，是 FlashAttention 的 CUDA kernel 或 Triton kernel 实现。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def flash_attention(q, k, v, block_size=64, dropout_p=0.0):\n",
    "    \"\"\"\n",
    "    简化版 FlashAttention 实现（前向传播）\n",
    "    输入:\n",
    "        q: [B, N, D] - Query\n",
    "        k: [B, N, D] - Key\n",
    "        v: [B, N, D] - Value\n",
    "        block_size: tile 大小\n",
    "        dropout_p: dropout 概率（用于训练）\n",
    "    返回:\n",
    "        output: [B, N, D]\n",
    "    \"\"\"\n",
    "    B, N, D = q.shape\n",
    "    scale = 1.0 / (D ** 0.5)\n",
    "    output = torch.zeros_like(q)\n",
    "    for i in range(0, N, block_size):\n",
    "        q_block = q[:, i:i+block_size]  # [B, Bq, D]\n",
    "        max_score = None\n",
    "        row_sum_exp = None\n",
    "        acc = torch.zeros_like(q_block)\n",
    "    \n",
    "        for j in range(0, N, block_size):\n",
    "            k_block = k[:, j:j+block_size]  # [B, Bk, D]\n",
    "            v_block = v[:, j:j+block_size]  # [B, Bk, D]\n",
    "    \n",
    "            # 1. Attention logits\n",
    "            scores = torch.bmm(q_block, k_block.transpose(1, 2)) * scale  # [B, Bq, Bk]\n",
    "    \n",
    "            # 2. Numerical stability\n",
    "            block_max = scores.max(dim=-1, keepdim=True).values  # [B, Bq, 1]\n",
    "            scores = scores - block_max\n",
    "            exp_scores = scores.exp()  # [B, Bq, Bk]\n",
    "    \n",
    "            # 3. Dropout (可选)\n",
    "            if dropout_p > 0.0:\n",
    "                exp_scores = F.dropout(exp_scores, p=dropout_p, training=True)\n",
    "    \n",
    "            # 4. Weighted sum\n",
    "            acc += torch.bmm(exp_scores, v_block)  # [B, Bq, D]\n",
    "    \n",
    "            # 5. Softmax normalization (log-sum-exp trick)\n",
    "            block_sum = exp_scores.sum(dim=-1, keepdim=True)  # [B, Bq, 1]\n",
    "            if row_sum_exp is None:\n",
    "                row_sum_exp = block_sum\n",
    "                max_score = block_max\n",
    "            else:\n",
    "                row_sum_exp += block_sum\n",
    "                max_score = torch.max(max_score, block_max)\n",
    "    \n",
    "        # Normalize accumulated result\n",
    "        output[:, i:i+block_size] = acc / (row_sum_exp + 1e-6)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "#main.py\n",
    "B, N, D = 2, 256, 64  # batch, seq_len, dim\n",
    "q = torch.randn(B, N, D, device='cuda')\n",
    "k = torch.randn(B, N, D, device='cuda')\n",
    "v = torch.randn(B, N, D, device='cuda')\n",
    "\n",
    "out = flash_attention(q, k, v, block_size=64)\n",
    "print(out.shape)  # [2, 256, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要测试效率，可以直接调用torch封装好的flashattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmha\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlashMHA\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# batch, seq_len, dim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "from flash_attn.modules.mha import FlashMHA\n",
    "import torch\n",
    "\n",
    "x = torch.randn(8, 512, 512, device='cuda')  # batch, seq_len, dim\n",
    "mha = FlashMHA(embed_dim=512, num_heads=8, device='cuda')\n",
    "output = mha(x)\n",
    "print(output.shape)  # [8, 512, 512]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
