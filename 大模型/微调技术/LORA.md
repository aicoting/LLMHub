# **LoRAï¼ˆLow-Rank Adaptationï¼‰è¯¦ç»†æ­¥éª¤è§£æ**
## **1. LoRA ç®€ä»‹**
LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§ **é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT, Parameter Efficient Fine-Tuningï¼‰** æŠ€æœ¯ï¼Œä¸»è¦ç”¨äº **å¤§æ¨¡å‹ï¼ˆå¦‚ Transformerï¼‰** çš„ä½ç§©é€‚é…ã€‚  
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
âœ… **å†»ç»“å¤§éƒ¨åˆ†æ¨¡å‹å‚æ•°**ï¼Œåªåœ¨ **ä½ç§©çŸ©é˜µ** ä¸Šè¿›è¡Œ **å¢é‡è®­ç»ƒ**ï¼Œå¤§å¹…é™ä½è®­ç»ƒæˆæœ¬å’Œå­˜å‚¨éœ€æ±‚ã€‚  
âœ… é€‚ç”¨äº **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰**ã€**è§†è§‰Transformerï¼ˆViTï¼‰** åŠå…¶ä»– **å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹**ã€‚

---

## **2. LoRA è®­ç»ƒæ­¥éª¤**
LoRA **ä¸»è¦ä½œç”¨äº Transformer çš„ Attention å±‚**ï¼Œå¹¶é‡‡ç”¨ **ä½ç§©çŸ©é˜µåˆ†è§£** æ¥æ›¿ä»£ä¼ ç»Ÿçš„å…¨é‡å‚æ•°æ›´æ–°ã€‚å…¶è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š

---

### **Step 1: å†»ç»“é¢„è®­ç»ƒæ¨¡å‹**
**ä¼ ç»Ÿçš„å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰** éœ€è¦è°ƒæ•´ **æ•´ä¸ª Transformer çš„æƒé‡**ï¼Œè€Œ LoRA **å†»ç»“æ‰€æœ‰åŸå§‹æƒé‡**ï¼Œåªåœ¨ **æ³¨æ„åŠ›å±‚çš„éƒ¨åˆ†å‚æ•°ä¸Šè¿›è¡Œè°ƒæ•´**ã€‚

```python
for param in model.parameters():
    param.requires_grad = False  # å†»ç»“æ‰€æœ‰å‚æ•°
```

ğŸ“Œ **å¥½å¤„**ï¼šé¿å…å¯¹å¤§æ¨¡å‹è¿›è¡Œå®Œæ•´çš„åå‘ä¼ æ’­ï¼Œé™ä½è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚

---

### **Step 2: æ›¿æ¢ Transformer çš„å…¨è¿æ¥å±‚**
åœ¨ Transformer çš„æ³¨æ„åŠ›å±‚ä¸­ï¼ŒæŸ¥è¯¢ï¼ˆ**Q**ï¼‰ã€é”®ï¼ˆ**K**ï¼‰ã€å€¼ï¼ˆ**V**ï¼‰é€šå¸¸ç”± **å…¨è¿æ¥å±‚ï¼ˆLinear Layerï¼‰** è®¡ç®—ï¼š

Q=XWQ,K=XWK,V=XWVQ = XW_Q, \quad K = XW_K, \quad V = XW_V

LoRA **ä¸ç›´æ¥è®­ç»ƒ** åŸå§‹çš„ `W_Q`ã€`W_K`ã€`W_V`ï¼Œè€Œæ˜¯ **å¯¹ **`**W_Q**`** è¿›è¡Œä¸€ä¸ªä½ç§©çŸ©é˜µè¿‘ä¼¼**ï¼š

Î”WQ=BA\Delta W_Q = BA

å…¶ä¸­ï¼š

+ **A**ï¼ˆå¤§å° `d Ã— r`ï¼‰ï¼šä½ç§©çŸ©é˜µï¼ˆr æ˜¯ç§©ï¼‰ã€‚
+ **B**ï¼ˆå¤§å° `r Ã— d`ï¼‰ï¼šå¦ä¸€ä¸ªä½ç§©çŸ©é˜µã€‚
+ **r â‰ª d**ï¼ˆè¿œå°äº dï¼‰ï¼Œé™ä½è®­ç»ƒå‚æ•°é‡ã€‚

### **ä»£ç ç¤ºä¾‹**
```python
import torch
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # åŸå§‹å…¨è¿æ¥å±‚ï¼ˆå†»ç»“ï¼‰
        self.W = nn.Linear(in_features, out_features, bias=False)
        self.W.requires_grad_(False)
        
        # LoRA ä½ç§©å‚æ•°
        self.A = nn.Linear(in_features, rank, bias=False)  # d Ã— r
        self.B = nn.Linear(rank, out_features, bias=False)  # r Ã— d
        
        # åˆå§‹åŒ–ä½ç§©çŸ©é˜µï¼ˆé›¶åˆå§‹åŒ–ï¼‰
        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)
        nn.init.zeros_(self.B.weight)
    
    def forward(self, x):
        return self.W(x) + self.alpha * self.B(self.A(x))  # ä½ç§©è¿‘ä¼¼
```

ğŸ“Œ **å…³é”®ç‚¹**ï¼š

+ `A` å’Œ `B` æ˜¯ä¸¤ä¸ª **ä½ç§©çŸ©é˜µ**ï¼Œåˆå§‹å€¼è®¾ä¸º **é›¶**ï¼ˆé˜²æ­¢å½±å“åŸå§‹æ¨¡å‹ï¼‰ã€‚
+ `alpha` æ˜¯ **ç¼©æ”¾å› å­**ï¼Œç”¨äºè°ƒæ•´ LoRA å½±å“ç¨‹åº¦ã€‚

---

### **Step 3: åªè®­ç»ƒ LoRA ä½ç§©å‚æ•°**
```python
optimizer = torch.optim.AdamW([
    {'params': model.lora_A.parameters()},
    {'params': model.lora_B.parameters()}
], lr=1e-4)
```

+ **åªè®­ç»ƒ **`**A**`** å’Œ **`**B**`**ï¼Œå†»ç»“åŸå§‹å‚æ•°**ã€‚
+ ç”±äº **r è¿œå°äº d**ï¼Œè®¡ç®—é‡å’Œå­˜å‚¨é‡å¤§å¹…é™ä½ã€‚

---

### **Step 4: ç»„åˆ LoRA å’ŒåŸå§‹ Transformer**
è®­ç»ƒå®Œæˆåï¼ŒLoRA è®¡ç®—çš„å¢é‡ `Î”W_Q = BA`**ç›´æ¥åŠ åˆ°** åŸå§‹ `W_Q` ä¸Šï¼š

Qâ€²=X(WQ+Î”WQ)Q' = X(W_Q + \Delta W_Q)

ğŸ“Œ **å¥½å¤„**ï¼š

+ **å¯ä»¥ç›´æ¥åˆå¹¶åˆ°åŸæ¨¡å‹æƒé‡**ï¼Œæ— éœ€é¢å¤–æ¨ç†å¼€é”€ã€‚
+ åªéœ€ **å°‘é‡é¢å¤–å‚æ•°** å³å¯å®Œæˆå¾®è°ƒã€‚

---

### **Step 5: æ¨ç†æ—¶çš„ LoRA**
LoRA è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥é€‰æ‹©ï¼š

1. **ä¿æŒ LoRA åˆ†è§£ç»“æ„ï¼ˆå‡å°‘å‚æ•°é‡ï¼‰**ï¼š
    - é€‚ç”¨äºå¤šä¸ª LoRA ä»»åŠ¡çš„åŠ¨æ€åˆ‡æ¢ã€‚
2. **åˆå¹¶ LoRA å‚æ•°åˆ°åŸæ¨¡å‹**ï¼š
    - é€‚ç”¨äºé«˜æ•ˆæ¨ç†ï¼Œé¿å…é¢å¤–è®¡ç®—å¼€é”€ï¼š

```python
model.W_Q.weight.data += model.B.weight @ model.A.weight
```

---

## **3. LoRA ç›¸æ¯”ä¼ ç»Ÿå¾®è°ƒçš„ä¼˜åŠ¿**
| **å¯¹æ¯”é¡¹** | **ä¼ ç»Ÿå¾®è°ƒï¼ˆFull Fine-Tuningï¼‰** | **LoRA** |
| --- | --- | --- |
| **å‚æ•°æ›´æ–°** | æ‰€æœ‰å‚æ•° | ä»… Q/K/V ä½ç§©çŸ©é˜µ |
| **è®­ç»ƒå¼€é”€** | é«˜ï¼ˆæ•°åäº¿å‚æ•°ï¼‰ | ä½ï¼ˆç™¾ä¸‡çº§åˆ«ï¼‰ |
| **å­˜å‚¨éœ€æ±‚** | å¤§ | å° |
| **æ¨ç†æ•ˆç‡** | å¯èƒ½å—å½±å“ | å½±å“å° |


ğŸ“Œ **LoRA é€‚ç”¨äº**ï¼š

+ **å¤§æ¨¡å‹å¾®è°ƒ**ï¼ˆå¦‚ GPTã€LLaMAã€ViTï¼‰ã€‚
+ **é«˜æ•ˆå­˜å‚¨ï¼ˆå¤šä¸ªä»»åŠ¡å¿«é€Ÿåˆ‡æ¢ï¼‰**ã€‚
+ **ç®—åŠ›å—é™çš„ç¯å¢ƒ**ï¼ˆå¦‚æ‰‹æœºç«¯ã€è¾¹ç¼˜è®¡ç®—ï¼‰ã€‚

---

## **4. LoRA åœ¨ Transformer ç»“æ„ä¸­çš„ä½ç½®**
![](https://raw.githubusercontent.com/microsoft/LoRA/main/images/lora_diagram.png)  
ğŸ“Œ **LoRA ä½œç”¨äº Multi-Head Attention çš„ Query/Key/Value è®¡ç®—ä¸­**ã€‚

---

## **5. LoRA ä»£ç ç¤ºä¾‹ï¼ˆå®Œæ•´è®­ç»ƒï¼‰**
```python
import torch
import torch.nn as nn
import torch.optim as optim

class LoRAModel(nn.Module):
    def __init__(self, d, r=4, alpha=32):
        super().__init__()
        self.W = nn.Linear(d, d, bias=False)  # å†»ç»“åŸå§‹æƒé‡
        self.W.requires_grad_(False)

        self.A = nn.Linear(d, r, bias=False)  # ä½ç§©çŸ©é˜µ A
        self.B = nn.Linear(r, d, bias=False)  # ä½ç§©çŸ©é˜µ B

        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)
        nn.init.zeros_(self.B.weight)

        self.alpha = alpha

    def forward(self, x):
        return self.W(x) + self.alpha * self.B(self.A(x))

# åˆ›å»ºæ¨¡å‹
model = LoRAModel(d=512, r=4).cuda()

# åªè®­ç»ƒ LoRA ä½ç§©å‚æ•°
optimizer = optim.AdamW([
    {'params': model.A.parameters()},
    {'params': model.B.parameters()}
], lr=1e-4)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    x = torch.randn(32, 512).cuda()
    y = model(x).sum()
    y.backward()
    optimizer.step()
    optimizer.zero_grad()
    print(f"Epoch {epoch}: Loss={y.item()}")
```

---

## **6. æ€»ç»“**
âœ… **LoRA ä¸»è¦ä¼˜åŒ– Transformer æ³¨æ„åŠ›å±‚**ï¼Œé€šè¿‡ **ä½ç§©åˆ†è§£** è¿‘ä¼¼ `W_Q`ã€`W_K`ã€`W_V`ï¼Œå¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°é‡ã€‚  
âœ… **å†»ç»“å¤§æ¨¡å‹æƒé‡**ï¼Œä»…è°ƒæ•´ **ä½ç§©çŸ©é˜µ**ï¼Œä½¿å¾— **å­˜å‚¨å’Œè®¡ç®—æˆæœ¬å¤§å¹…ä¸‹é™**ã€‚  
âœ… **é€‚ç”¨äº LLMï¼ˆGPTã€BERTã€LLaMAï¼‰ç­‰å¤§æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒ**ï¼Œæ”¯æŒ **å¤šä»»åŠ¡åˆ‡æ¢** å’Œ **é«˜æ•ˆæ¨ç†**ã€‚



### 
