# ğŸ“š å¾®è°ƒç³»åˆ—æ–‡ç« 
[ä¸€æ–‡äº†è§£å¾®è°ƒæŠ€æœ¯çš„å‘å±•ä¸æ¼”è¿›](https://zhuanlan.zhihu.com/p/1939080284374022103)  
[ä¸€æ–‡ææ‡‚ LoRA å¦‚ä½•é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹](https://zhuanlan.zhihu.com/p/1939447022114567022)

éšç€å¤§è§„æ¨¡ Transformer æ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMAã€ViTï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå¾®è°ƒå¤§æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬æˆä¸ºåˆ¶çº¦å› ç´ ã€‚  
LoRA ä½œä¸ºä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£ï¼Œä»…å¾®è°ƒå¢é‡éƒ¨åˆ†ï¼Œæœ‰æ•ˆé™ä½äº†èµ„æºæ¶ˆè€—ã€‚

æœ¬æ–‡å°†åˆ†æ­¥éª¤è§£æ LoRA çš„è®­ç»ƒåŸç†åŠä¼˜åŠ¿ï¼Œå¸®åŠ©ä½ å¿«é€ŸæŒæ¡ LoRA çš„æ ¸å¿ƒæœºåˆ¶ã€‚

<font style="color:rgb(25, 27, 31);">æ‰€æœ‰ç›¸å…³æºç ç¤ºä¾‹ã€æµç¨‹å›¾ã€æ¨¡å‹é…ç½®ä¸çŸ¥è¯†åº“æ„å»ºæŠ€å·§ï¼Œæˆ‘ä¹Ÿå°†æŒç»­æ›´æ–°åœ¨Githubï¼š</font>[**<font style="color:rgb(25, 27, 31);">LLMHub</font>**](https://github.com/aicoting/LLMHub)<font style="color:rgb(25, 27, 31);">ï¼Œæ¬¢è¿å…³æ³¨æ”¶è—ï¼</font>

---

## ä¸€ã€LoRA ç®€ä»‹
![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1754983348216-9c00e71e-39c8-473d-8486-31157df578ef.png)

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§é’ˆå¯¹å¤§æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

+ **å†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œåªåœ¨ä½ç§©çŸ©é˜µä¸Šè¿›è¡Œå¢é‡è®­ç»ƒ**ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬ï¼›
+ é€‚ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è§†è§‰ Transformerï¼ˆViTï¼‰åŠå…¶ä»–å¤§è§„æ¨¡æ·±åº¦æ¨¡å‹ã€‚

---

## äºŒã€LoRA è®­ç»ƒè¯¦ç»†æ­¥éª¤
### å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°
ä¼ ç»Ÿå¾®è°ƒéœ€è°ƒæ•´æ•´ä¸ª Transformer æƒé‡ï¼Œè€Œ LoRA åªå†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œé¿å…å…¨é‡åå‘ä¼ æ’­å¼€é”€ã€‚

```python
for param in model.parameters():
    param.requires_grad = False  # å†»ç»“æ‰€æœ‰å‚æ•°
```

ä¼˜ç‚¹ï¼šæ˜¾è‘—é™ä½è®­ç»ƒè®¡ç®—å’Œæ˜¾å­˜éœ€æ±‚ã€‚

---

### æ›¿æ¢ Transformer æ³¨æ„åŠ›å±‚çš„å…¨è¿æ¥å±‚
Transformer ä¸­ï¼ŒæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰è®¡ç®—é€šå¸¸é€šè¿‡çº¿æ€§å±‚å®ç°ï¼š

![image](https://cdn.nlark.com/yuque/__latex/c9c465ad77b490bd7af8ebfe38bafbc8.svg)

LoRA ä¸ç›´æ¥è®­ç»ƒåŸå§‹æƒé‡ ![image](https://cdn.nlark.com/yuque/__latex/60b59ab950cf235a6c25eb186a35ee5d.svg)ï¼Œè€Œæ˜¯å¯¹å…¶å¢é‡è¿›è¡Œä½ç§©åˆ†è§£ï¼š

![image](https://cdn.nlark.com/yuque/__latex/a5cdcffd6669ef95358de22c7d0e6eac.svg)

å…¶ä¸­ï¼š

+ ![image](https://cdn.nlark.com/yuque/__latex/de951302f41d4707b9d80ca1af34dd0f.svg) å¤§å°ä¸º ![image](https://cdn.nlark.com/yuque/__latex/c5a0dfe09e71eaea8635ffd525bf3f56.svg)ï¼ˆä½ç§©çŸ©é˜µï¼‰ï¼Œ
+ ![image](https://cdn.nlark.com/yuque/__latex/54f5fb1b07a88521e7b036e3bc7a5e33.svg) å¤§å°ä¸º ![image](https://cdn.nlark.com/yuque/__latex/a3c057578ca804946db284d512f2653b.svg)ï¼Œ
+ ![image](https://cdn.nlark.com/yuque/__latex/02348d660d6f257d77d8965fffa03b34.svg)ï¼Œå¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°ã€‚

ä»£ç ç¤ºä¾‹ï¼š

```python
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha

        self.W = nn.Linear(in_features, out_features, bias=False)
        self.W.requires_grad_(False)  # å†»ç»“åŸæƒé‡

        self.A = nn.Linear(in_features, rank, bias=False)  # d Ã— r
        self.B = nn.Linear(rank, out_features, bias=False)  # r Ã— d

        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)
        nn.init.zeros_(self.B.weight)  # B é›¶åˆå§‹åŒ–ï¼Œé˜²æ­¢æ‰°åŠ¨åŸå§‹æƒé‡

    def forward(self, x):
        return self.W(x) + self.alpha * self.B(self.A(x))
```

---

### åªè®­ç»ƒä½ç§©çŸ©é˜µå‚æ•°
```python
optimizer = torch.optim.AdamW([
    {'params': model.lora_A.parameters()},
    {'params': model.lora_B.parameters()}
], lr=1e-4)
```

ä»…è®­ç»ƒ ![image](https://cdn.nlark.com/yuque/__latex/de951302f41d4707b9d80ca1af34dd0f.svg) å’Œ ![image](https://cdn.nlark.com/yuque/__latex/54f5fb1b07a88521e7b036e3bc7a5e33.svg)ï¼Œå†»ç»“åŸæ¨¡å‹æ‰€æœ‰å‚æ•°ï¼Œæ˜¾è‘—é™ä½è®¡ç®—é‡ã€‚

---

### è®­ç»ƒå®Œæˆåæƒé‡åˆå¹¶
è®­ç»ƒå®Œæˆåï¼Œå¯å°†å¢é‡æƒé‡ç›´æ¥åŠ åˆ°åŸå§‹æƒé‡ï¼š

![image](https://cdn.nlark.com/yuque/__latex/9f7d5e07025987e0d8fb8e1f3b69c16f.svg)

åˆå¹¶ä¼˜åŠ¿ï¼š

+ æ¨ç†æ—¶æ— é¢å¤–è®¡ç®—å¼€é”€ï¼›
+ è½»æ¾éƒ¨ç½²ï¼Œæ— éœ€ä¿ç•™é¢å¤–å‚æ•°ç»“æ„ã€‚

---

### æ¨ç†é˜¶æ®µé€‰æ‹©
+ **ä¿æŒ LoRA ç»“æ„**ï¼šé€‚åˆå¤šä»»åŠ¡åŠ¨æ€åˆ‡æ¢ï¼ŒèŠ‚çœå­˜å‚¨ï¼›
+ **åˆå¹¶æƒé‡**ï¼šé€‚åˆå•ä¸€ä»»åŠ¡é«˜æ•ˆæ¨ç†ï¼Œé¿å…é¢å¤–è®¡ç®—ã€‚

ç¤ºä¾‹åˆå¹¶ä»£ç ï¼š

```python
model.W_Q.weight.data += model.B.weight @ model.A.weight
```

---

## ä¸‰ã€LoRA ä¸ä¼ ç»Ÿå¾®è°ƒå¯¹æ¯”
| å¯¹æ¯”é¡¹ | ä¼ ç»Ÿå¾®è°ƒï¼ˆFull Fine-Tuningï¼‰ | LoRA å¾®è°ƒ |
| --- | --- | --- |
| å‚æ•°æ›´æ–° | å…¨éƒ¨æƒé‡ | ä»…ä½ç§©çŸ©é˜µ A å’Œ B |
| è®­ç»ƒå¼€é”€ | é«˜ï¼ˆæ•°åäº¿å‚æ•°ï¼‰ | ä½ï¼ˆç™¾ä¸‡å‚æ•°çº§åˆ«ï¼‰ |
| å­˜å‚¨éœ€æ±‚ | å¤§ | å° |
| æ¨ç†æ•ˆç‡ | å¯èƒ½å—å½±å“ | å‡ ä¹æ— é¢å¤–è´Ÿæ‹… |


LoRA ç‰¹åˆ«é€‚åˆï¼š

+ å¤§è§„æ¨¡ Transformer å¾®è°ƒï¼ˆå¦‚ GPTã€LLaMAã€ViTï¼‰ï¼›
+ å¤šä»»åŠ¡æ¨¡å‹å¿«é€Ÿåˆ‡æ¢ä¸å­˜å‚¨ä¼˜åŒ–ï¼›
+ ç®—åŠ›å—é™ç¯å¢ƒï¼Œå¦‚ç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¡ç®—ã€‚

---

## å››ã€LoRA åœ¨ Transformer ä¸­çš„ä½œç”¨ä½ç½®
LoRA ä¸»è¦ä½œç”¨äº Multi-Head Attention çš„æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰çº¿æ€§å±‚ï¼Œæ˜¯å¾®è°ƒæœ€å…³é”®çš„å‚æ•°éƒ¨åˆ†ã€‚

---

## äº”ã€LoRA è®­ç»ƒç¤ºä¾‹ä»£ç 
```python
import torch
import torch.nn as nn
import torch.optim as optim

class LoRAModel(nn.Module):
    def __init__(self, d, r=4, alpha=32):
        super().__init__()
        self.W = nn.Linear(d, d, bias=False)
        self.W.requires_grad_(False)

        self.A = nn.Linear(d, r, bias=False)
        self.B = nn.Linear(r, d, bias=False)

        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)
        nn.init.zeros_(self.B.weight)

        self.alpha = alpha

    def forward(self, x):
        return self.W(x) + self.alpha * self.B(self.A(x))

model = LoRAModel(d=512, r=4).cuda()

optimizer = optim.AdamW([
    {'params': model.A.parameters()},
    {'params': model.B.parameters()}
], lr=1e-4)

for epoch in range(10):
    x = torch.randn(32, 512).cuda()
    y = model(x).sum()
    y.backward()
    optimizer.step()
    optimizer.zero_grad()
    print(f"Epoch {epoch}: Loss={y.item()}")
```

---

## å…­ã€æ€»ç»“
LoRA é€šè¿‡å¯¹ Transformer æ³¨æ„åŠ›å±‚æƒé‡å¢é‡è¿›è¡Œä½ç§©åˆ†è§£ï¼Œæœ‰æ•ˆå‡å°‘è®­ç»ƒå‚æ•°é‡å’Œè®¡ç®—èµ„æºæ¶ˆè€—ã€‚å…¶é‡‡ç”¨å†»ç»“å¤§æ¨¡å‹å‚æ•°ï¼Œä»…è®­ç»ƒä½ç§©çŸ©é˜µï¼Œé™ä½å­˜å‚¨å’Œè®¡ç®—å¼€é”€ã€‚å¹¶ä¸”LORAæ”¯æŒå¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰ Transformer çš„é«˜æ•ˆå¾®è°ƒï¼Œå¯ä»¥å…¼é¡¾å¤šä»»åŠ¡å’Œå¿«é€Ÿæ¨ç†ã€‚



<font style="color:rgb(25, 27, 31);">å…³äºæ·±åº¦å­¦ä¹ å’Œå¤§æ¨¡å‹ç›¸å…³çš„çŸ¥è¯†å’Œå‰æ²¿æŠ€æœ¯æ›´æ–°ï¼Œè¯·å…³æ³¨å…¬ä¼—å·</font><font style="color:rgb(25, 27, 31);background-color:rgb(246, 246, 246);">coting</font><font style="color:rgb(25, 27, 31);">ï¼</font>



