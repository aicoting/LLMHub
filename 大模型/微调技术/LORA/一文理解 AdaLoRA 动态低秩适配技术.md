# 📚 微调系列文章
[一文了解微调技术的发展与演进](https://zhuanlan.zhihu.com/p/1939080284374022103)  
[一文搞懂 LoRA 如何高效微调大模型](https://zhuanlan.zhihu.com/p/1939447022114567022)  
[LoRA详细步骤解析](https://zhuanlan.zhihu.com/p/1939807872113410970)	  
[一文搞懂如何用 QLoRA 高效微调大语言模型](https://zhuanlan.zhihu.com/p/1939997552779978284)



随着大规模语言模型的不断壮大，如何在有限资源下高效微调模型，成为研究热点。  
传统 LoRA 固定低秩大小，难以兼顾所有层的复杂性和任务需求。  
AdaLoRA（Adaptive LoRA）创新地引入动态调整秩的机制，根据层的重要性和训练过程自动分配参数资源，提升微调效果和效率。

在了解 AdaLoRA 之前，可以带着这三个问题阅读本文：

1. **固定秩的 LoRA 有哪些局限？**
2. **AdaLoRA 是如何实现动态秩分配的？**
3. **采用 AdaLoRA 微调时，应注意哪些关键点？**

****

<font style="color:rgb(25, 27, 31);">所有相关源码示例、流程图、模型配置与知识库构建技巧，我也将持续更新在Github：</font>[**<font style="color:rgb(25, 27, 31);">LLMHub</font>**](https://github.com/aicoting/LLMHub)<font style="color:rgb(25, 27, 31);">，欢迎关注收藏！</font>

---

## 一、背景与挑战
LoRA 通过低秩矩阵分解实现参数高效微调，显著降低训练成本。  
但其固定秩 ![image](https://cdn.nlark.com/yuque/__latex/72cb3a229067770aeb6caa625a65a1a1.svg) 设计对所有层一视同仁，无法针对不同层和任务灵活调整参数预算。

这带来两大问题：

+ **资源浪费**：部分简单层过度分配参数；
+ **性能瓶颈**：复杂层参数不足，限制模型表达能力。

因此，动态、按需分配秩大小的需求日益凸显。

---

## 二、AdaLoRA 的核心技术原理
AdaLoRA 通过引入 **动态秩调整机制**，实现训练中对各层低秩参数的自适应分配。

具体来说，AdaLoRA采用**奇异值分解**（SVD）对增量矩阵进行参数化，利用重要性指标剪枝低影响的奇异值，同时保留奇异向量。由于完整SVD计算代价极高，该方法通过**降低参数预算**来加快计算过程，并保持后续恢复的可能性，保证训练稳定。

此外，为了避免繁重的SVD计算，AdaLoRA在训练损失中加入正交性约束，规范奇异矩阵 P和 Q，从而简化训练过程，提升模型稳定性。

### 核心思想
+ 训练过程中监控各层低秩矩阵的重要性指标（如梯度、权重幅度等）；
+ 根据指标自动调整每层秩大小，释放更多参数到关键层，减少不重要层参数；
+ 通过**稀疏化约束**和**正则化**保证秩调整的稳定与收敛。

### 技术流程
1. 初始化各层**低秩矩阵**，秩大小设置为最大值；
2. 训练过程中**动态调整秩**，剪枝不重要的秩通道；
3. 定期评估并重分配秩大小，实现参数预算的最优分配；
4. 训练结束时，获得各层最优秩配置。

这样，模型能够在有限参数预算下，发挥最大潜力。

---

## 三、AdaLoRA 的优势与适用场景
![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1755265594642-70b4172f-9d63-4c40-a888-f3764d35f29f.png)

+ **提升参数利用率**：动态分配避免参数浪费，增强模型表达能力。
+ **适应复杂多变任务**：不同任务对模型层需求差异大，AdaLoRA 能自动调节适应多任务训练。
+ **减小显存占用**：通过剪枝无用秩通道，降低显存和计算负担。
+ **兼容主流微调框架**：可与 PEFT、QLoRA 等结合，扩展微调技术栈。

适用场景包括：

+ 资源受限但需高性能模型的微调任务；
+ 多任务、多领域模型训练；
+ 需要自动参数调节的复杂训练环境。

---

## 四、AdaLoRA使用建议
+ **秩调整频率**  
需合理设定动态调整的间隔，避免训练不稳定。
+ **重要性指标选择**  
依据梯度范数、权重幅度等设计指标，确保准确反映参数贡献。
+ **正则化设计**  
加入适当正则项防止过度剪枝或秩膨胀。
+ **训练超参调优**  
关注剪枝阈值、最大秩限制及学习率调节。

---

最后我们回答一下开头的三个问题

1. **固定秩的 LoRA 有哪些局限？**  
固定秩无法灵活适应不同层的需求，导致部分层参数浪费，另一些关键层参数不足，影响微调效果和资源利用效率。
2. **AdaLoRA 是如何实现动态秩分配的？**  
AdaLoRA根据梯度或权重幅度动态调整每层低秩矩阵大小，剪枝不重要通道，将参数资源集中到关键层，实现更优的参数利用。
3. **采用 AdaLoRA 微调时，应注意哪些关键点？**  
需合理设置调整频率、选择准确的重要性指标并加入正则防止过度剪枝，同时调节超参数保证训练稳定和微调效果。

[https://arxiv.org/pdf/2303.10512](https://arxiv.org/pdf/2303.10512)



<font style="color:rgb(25, 27, 31);">关于深度学习和大模型相关的知识和前沿技术更新，请关注公众号</font><font style="color:rgb(25, 27, 31);background-color:rgb(246, 246, 246);">coting</font><font style="color:rgb(25, 27, 31);">！</font>



