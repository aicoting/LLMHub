大语言模型（LLM）如 ChatGPT、Claude、Gemini 等，正以前所未有的方式改变我们的工作与生活。但在享受它们带来的便利时，我们也越来越频繁地遇到一个令人困惑的问题：**它们有时会一本正经地“胡说八道”**，仿佛陷入了幻觉。

比如：

> “帮我找一篇关于‘Prompt Tuning’的论文。”  
模型回答：“请参考《Prompt Tuning for Few-Shot Learning》，作者为J. Smith，发表于Nature 2021。”  
——结果：这篇论文压根不存在！
>



![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1752248325842-7d820196-cb18-4804-8906-e4a8fbf83fe1.png)

这类现象，被称为**幻觉（Hallucination）**，是当前大模型在实际应用中最广泛、最头疼的问题之一。

---

## 一、什么是大模型幻觉？
**幻觉**，在大语言模型中指的是：模型生成的内容**在语法和逻辑上看似合理，但在事实层面却是虚假或错误的**。

这不仅限于虚构的引用或文献，还包括：

+ 错误的数据或统计结果
+ 虚构的公司、人物、地名
+ 时间顺序错乱
+ 概念或定义张冠李戴

换句话说，幻觉是一种**“听起来像真的，其实是假的”**现象。

---

## 二、大模型为什么会产生幻觉？
幻觉并不是 bug，而是大模型的**机制使然**。它背后有几个根本原因：

### 1. 语言模型本质是“预测”而非“理解”
大语言模型的基本机制是：**在已知上下文的情况下预测下一个最可能出现的词（token）**。比如：

```plain
我今天去了北京天安门广场，那里有很多 →
```

模型很可能预测出“游客”、“人群”、“活动”等词，这是因为训练中它见过类似的句式。**但这种预测是基于语言模式，并不要求内容一定真实**。因此，模型可能“编”出一个听起来合理、但事实错误的回答。

### 2. 缺乏事实核查机制
目前大多数 LLM 都是“闭环生成”：输入 → 生成答案，不会对结果进行事实查证。它们不像人类会“回头查资料”验证正确性。

### 3. 训练数据本身可能不准确
LLM 的训练数据来源于海量的网络文本，其中不可避免地包含：

+ 虚假或过时信息
+ 主观性强、缺乏事实依据的内容（比如论坛、评论）
+ 某些领域数据严重缺乏（如小语种医学资料）

这些都会为幻觉埋下伏笔。

### 4. 模型目标函数没有“真实性”维度
大多数 LLM 训练时采用最大似然估计（MLE）作为目标，只优化“预测概率最大”而非“信息最真实”。

换句话说，模型更关心“说得像不像”，而不是“说得对不对”。

---

## 三、幻觉的常见表现形式
不同场景下，幻觉的表现方式可能千差万别，主要包括以下几类：

### 1. 虚构事实
+ 编造论文标题、学术会议、作者
+ 杜撰人物、组织、历史事件

> 示例：捏造一个“2023年诺贝尔数学奖”，而实际该奖项并不存在
>

### 2. 时间错乱
+ 把2020年的事件说成是“最近发生的”
+ 混淆历史顺序

### 3. 引用错误
+ 给出的书籍或论文并不存在
+ 混淆作者或出处

### 4. 数值与逻辑错误
+ 算术错误
+ 无法正确处理统计数据、逻辑推理任务

---

## 四、如何评估和检测幻觉？
当前对于大模型幻觉的检测，既有**自动化手段**，也有**人工干预**。主要包括：

### 1. 自动评估指标
+ **Factual Consistency**：判断生成文本是否与给定事实一致
+ **FactScore、FEVER Score**：结合知识库评估生成文本的真实性
+ **GPT-judge / SelfCheckGPT**：使用另一个模型来交叉验证生成内容是否存在幻觉

这些方法尚不成熟，对开放场景、长文本、推理内容仍存在偏差。

### 2. 人工评估
人工评估是目前最可靠但最费力的方式。尤其在医学、法律等高风险场景中，**需要专家团队进行事实核对**。

### 3. 结合知识库的评估
通过将生成结果与结构化知识库（如Wikipedia、PubMed、Wikidata）进行比对，能有效识别一些虚构信息。这也推动了“知识增强生成”（如RAG）的发展。

---

## 五、应对幻觉的策略
虽然幻觉无法完全避免，但已有很多实用的方法能大幅减少它的发生概率：

### 1. 提示工程（Prompt Engineering）
一个好提示往往能极大降低幻觉率。比如：

+ 加上“请基于已知事实”或“如不确定请不要回答”
+ 使用few-shot examples让模型模仿可信回答风格

### 2. 检索增强生成（RAG）
在生成之前或过程中引入外部检索系统，模型仅基于查到的“真实材料”生成内容，可以显著降低幻觉率。例如：

> 给模型一个文献摘要，它再基于摘要回答问题。
>

我之前有出过一个系列专门介绍RAG。

+ <font style="color:rgb(25, 27, 31);">RAG 实战指南（一）：</font>[什么是RAG？一文搞懂检索增强生成技术](https://zhuanlan.zhihu.com/p/1912270367357122436)
+ <font style="color:rgb(25, 27, 31);">RAG 实战指南（二）：</font>[一文搞懂RAG 的文档解析](https://zhuanlan.zhihu.com/p/1912549174966194672)
+ <font style="color:rgb(25, 27, 31);">RAG 实战指南（三）：</font>[一文搞懂RAG 的切分策略](https://zhuanlan.zhihu.com/p/1912878600853623201)
+ <font style="color:rgb(25, 27, 31);">RAG 实战指南（四）：</font>[RAG-embedding篇](https://zhuanlan.zhihu.com/p/1912910452339484544)
+ <font style="color:rgb(25, 27, 31);">RAG 实战指南（五）：</font>[RAG信息检索-如何让模型找到‘对的知识’](https://zhuanlan.zhihu.com/p/1912920089109430794)

### 3. 多模型协同校验
一个模型生成内容，另一个模型验证真假。比如Bard和Gemini尝试加入验证阶段，或引入“二次判别器”结构。

### 4. 改进训练流程
通过引入：

+ 强化学习（RLHF），利用人类反馈优化生成质量，后续也会和大家一起学习RLHF！
+ 更干净、结构化的训练数据
+ 引入事实对比或知识记忆模块  
可以在根源上减少模型“编造”的倾向。



