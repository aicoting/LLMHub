# 基础知识点
## 梯度消失和梯度爆炸的原因是什么  
### 问题分析
梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）本质上都是在深层神经网络中反向传播过程中，梯度在多层传播时逐渐缩小或放大的问题，导致模型难以有效训练。

#### **1. 梯度消失的原因**
+ **链式法则累乘导致梯度趋近于0**  
在反向传播时，梯度是各层导数的连乘。如果激活函数（如sigmoid或tanh）的导数小于1，连续相乘后梯度迅速趋近0，导致前面层几乎不更新（学习停滞）。
+ **激活函数饱和**  
比如sigmoid在输入过大或过小时梯度接近0，进一步加剧梯度消失。
+ **权重初始化不合理**  
初始化权重过小也会导致信号逐步缩小。

#### **2. 梯度爆炸的原因**
+ **链式法则累乘导致梯度发散**  
如果激活函数的导数或权重大于1，连乘后梯度指数级增大，导致模型权重更新剧烈，甚至数值溢出。
+ **权重初始化过大**  
初始权重分布过大放大了信号。

### **面试回答**
梯度消失和梯度爆炸都是因为反向传播过程中梯度在多层传播时不断连乘，如果连乘的导数小于1就导致梯度消失，大于1则导致梯度爆炸。典型原因包括激活函数饱和（比如sigmoid）、权重初始化不合理和网络过深。

## 解决梯度消失和梯度爆炸的方法
### 问题分析
**“梯度消失和梯度爆炸是神经网络训练中常见的问题，主要出现在深层网络或RNN中。针对这两个问题，通常从以下几个方面入手进行处理：”**

#### 1. **权重初始化**
+ 使用合适的初始化方式可以缓解梯度问题：
    - 对于ReLU：使用 **He初始化**；
    - 对于sigmoid或tanh：使用 **Xavier初始化**。

#### 2. **使用合适的激活函数**
+ 避免使用容易饱和的激活函数（如 sigmoid）；
+ 优先使用 **ReLU** 及其变种（如 LeakyReLU, ELU），能有效缓解梯度消失。

#### 3. **使用归一化技术**
+ **Batch Normalization**：能使每一层的输入保持稳定，减缓梯度爆炸或消失；
+ **LayerNorm**（尤其适用于RNN、Transformer结构）。

#### 4. **梯度裁剪（Gradient Clipping）**
+ **主要用于RNN、LSTM**，在梯度过大时将其限制在某一范围内（如[-5, 5]）；

#### 5. **合理设置网络结构**
+ 减少不必要的层数；
+ 使用残差连接（Residual Connection），如 ResNet，有效缓解梯度消失。

#### 6. **优化器的选择**
+ 像 **Adam、RMSProp** 等自适应优化器，对梯度问题有一定缓解作用；

---

### 面试回答
**面对梯度问题，可以从初始化、激活函数、归一化、结构设计等多方面综合考虑，确保网络训练稳定高效。**

## 残差连接如何解决梯度爆炸和梯度消失
---

### **问题分析**
1. **残差连接是什么？**  
残差连接是在神经网络中引入的一种跳跃式连接方式，其公式为：

![image](https://cdn.nlark.com/yuque/__latex/ad468cbdf005efb70b3060f37bc505c9.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/a3803c9b72a55f8139079eb36aa883da.svg) 是要学习的残差函数（通常是若干个卷积层、ReLU等操作），![image](https://cdn.nlark.com/yuque/__latex/2adf48e1cf80858f0604d2b51eeed4b8.svg) 是输入，直接加到输出上。

2. **梯度消失/爆炸的本质原因**  
在传统深层神经网络中，梯度反向传播时需要连续乘多个梯度项（链式法则），如果每层的梯度值都小于 1（或大于 1），多次乘积会导致：
+ 梯度越来越小 → **梯度消失**
+ 梯度越来越大 → **梯度爆炸**
3. **残差连接如何缓解？**
+ 反向传播过程中，残差连接引入的恒等映射提供了**直接路径**，使得梯度可以“绕过”复杂的非线性变换，直接传播到更浅层，避免了长路径上的梯度连乘效应。

数学上，反向传播梯度可以表示为：

![image](https://cdn.nlark.com/yuque/__latex/3b30ed078fa983a19c63198030ca8ba1.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/df53f70d20a6a901dfd6da9f6986b470.svg) 是单位矩阵，表示恒等映射。  
即使 ![image](https://cdn.nlark.com/yuque/__latex/fb2e520265a45dd53d8aa560e0b96813.svg) 很小或很大，**“+I”** 保证了梯度至少可以稳定地传递下去。

4. **实际效果**
+ 使得网络更容易训练更深（比如 ResNet 可以训练 152 层以上的网络）
+ 缓解梯度消失，网络不会因为深度增加而性能下降（**退化问题 degradation problem**）

---

### **面试回答**
> 残差连接（Residual Connection）通过引入**恒等映射（identity mapping）**，使梯度在反向传播时可以直接沿跳跃路径（shortcut）传播，从而缓解了梯度消失和梯度爆炸问题，特别是在深层网络中，残差结构可以确保梯度不会因为连续非线性变换而过度缩小或放大，从而使深层网络更容易优化和训练。
>

---



## 归一化的作用
---

### **问题分析**
1. **梯度消失/爆炸产生的本质原因**
+ 神经网络每一层的输出经过非线性函数和权重变换，如果输入分布不稳定（方差过大或过小），经过多层之后，激活值和梯度可能迅速缩小或扩大 → 造成梯度消失或爆炸。
+ 特别在深层网络中，这种问题更严重。
2. **归一化怎么做的？**  
以 **Batch Normalization** 为例，对每一层的特征做：

![image](https://cdn.nlark.com/yuque/__latex/271c3576cd94fee8a3c312f44a650190.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/756a643380ff53c0692dbc2e7e930a35.svg) 和 ![image](https://cdn.nlark.com/yuque/__latex/217d731c58118430ccbb4f9f6d44ce08.svg) 是 mini-batch 中特征的均值和方差。

→ 结果：让激活值分布均值为 0，方差为 1。  
→ 激活值不会因为输入变化剧烈而偏离合理范围。

3. **为什么归一化可以缓解梯度消失/爆炸？**
+ **激活值保持稳定**：归一化限制了每一层输出的分布范围，不会快速放大/缩小，避免了激活值爆炸或接近 0 区域（比如 Sigmoid 饱和区）
+ **梯度保持稳定**：反向传播时，梯度与激活值直接相关，激活值稳定 → 梯度不会快速衰减或爆炸
+ **降低内部协变量偏移（Internal Covariate Shift）**：每层输入分布稳定，训练更稳定，学习率可以设得更大，加速收敛

---

### **面试回答**
> 归一化（Normalization）通过把每层输入或特征缩放到均值为 0、方差为 1 的分布，使得网络中的激活值保持在合理范围，从而防止梯度在前向传播或反向传播过程中因为数值过大或过小导致梯度爆炸或梯度消失。此外，归一化让每层的输入分布更稳定，有助于加速收敛并稳定训练。
>

---



## 为什么sigmoid会饱和  
![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1747120472018-240fada7-014d-4842-8903-a886b11cc0da.png)

## BatchNorm和LayerNorm有什么区别  
### 问题分析
| **维度** | **BatchNorm (BN)** | **LayerNorm (LN)** |
| --- | --- | --- |
| **归一化维度** | **样本间归一化**（batch维度） | **样本内归一化**（feature维度） |
| **计算公式** | 对每个特征维度 kkk 计算   μ和σ来自**batch中所有样本** | 对每个样本独立计算   μ和σ来自**特征维度** |
| **依赖 Batch Size** | **依赖 batch size**（小 batch 时效果差） | **不依赖 batch size** |
| **应用场景** | CNN、MLP 中常用 | Transformer、RNN 中常用 |
| **公式示例** | ![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1747120558875-3c870317-8fda-475f-8ece-dd7c4937bf80.png) | ![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1747120568287-867c030b-3b85-4d68-89b0-8d2233db2ffd.png) |
| **运行时统计（推理阶段）** | 需要保存 moving mean/var | 不需要额外统计 |


---

### **面试回答**
BatchNorm 对 batch 维度归一化，依赖多个样本，适合 CNN，但对小 batch 敏感；LayerNorm 对单个样本特征归一化，不依赖 batch size，更适合 RNN 和 Transformer 等序列模型。



## dropout的作用是什么  
### **问题分析**
**Dropout 是一种正则化方法，主要作用是防止过拟合 (overfitting)。**

#### **核心原理**
+ 训练时，**以一定概率随机丢弃神经元**（即设为0），让模型不能过度依赖某些特征
+ 这样每次训练都相当于在不同的「子网络」上更新参数  
→ **相当于集成多个模型 (model ensemble)**  
→ 提升模型的泛化能力

#### **数学表达**
训练时：

x~=x⋅m,m∼Bernoulli(p)

p为保留概率 (keep probability)

推理时：

+ 不丢节点，输出乘以 ppp 的缩放补偿（保持期望一致）

---

### **面试回答**
Dropout 通过训练时随机丢弃神经元，防止模型过度依赖局部特征，相当于集成多个子网络，主要作用是降低过拟合、提升泛化能力。



### **如果面试官追问「为什么 Dropout 能防止过拟合？」**
因为 Dropout 迫使网络学到冗余且更鲁棒的特征表达，不依赖单一神经元，同时相当于集成多个子模型，增强模型稳定性。

## 什么是神经元和激活函数
### **面试回答**
神经元是神经网络的基本计算单元，主要作用是对输入特征加权整合并引入非线性，从而帮助模型提取和组合复杂特征。在不同网络中，神经元形式不同：MLP 中是全连接，CNN 中是局部感受野，RNN 中有时间依赖，Transformer 结合自注意力建模全局关系。

---

### **为什么要引入激活函数**
没有激活函数，神经网络只是线性模型，无法拟合复杂非线性问题；激活函数使模型具有非线性表达能力，从而增强模型拟合能力。



## 池化的作用是什么  
### 问题分析
池化（**Pooling**）是卷积神经网络（CNN）中的一种重要操作，主要用于减少空间尺寸、控制计算量、提高模型的鲁棒性，且常用于 **卷积层** 和 **全连接层** 之间。其核心作用是通过汇聚特征图中的信息，减少参数数量和计算复杂度，并增强模型的泛化能力。

---

#### 池化的主要作用
1. **降低计算复杂度**
    - 池化操作通过减小特征图的空间维度（例如，从 ![image](https://cdn.nlark.com/yuque/__latex/a11ea46280669ac0bf8afdac4019d77f.svg) 降到 ![image](https://cdn.nlark.com/yuque/__latex/5c8213948b3250fd57bd3a27a4232c6a.svg)），**减少了后续层的计算量**。
    - 这样可以减少内存消耗并加速训练过程。
2. **控制过拟合**
    - 池化层通过降低特征图的分辨率，**使得模型更具鲁棒性**，降低了对局部特征的依赖，间接地对模型起到了正则化作用，从而减少了过拟合的风险。
3. **保留重要特征**
    - 池化操作能够聚焦图像中的重要信息，并丢弃较不重要的部分，使得卷积网络更加专注于图像的关键信息。
    - 常见的池化操作（如最大池化）通过选取局部区域内的最大值来保留重要特征。
4. **增强模型的平移不变性**
    - 池化可以让网络对输入图像的平移产生较小的敏感度，提升了**平移不变性**。这意味着网络对图像中的小的平移变换不再过于敏感，能够更好地处理图像中位置的变化。

---

#### 常见池化类型
1. **最大池化（Max Pooling）**
    - 对局部区域（通常为 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 或 ![image](https://cdn.nlark.com/yuque/__latex/b08d861763b277c3ff6c7f92bc62b06c.svg)）中的最大值进行池化。
    - **作用**：保留该区域的最显著特征，增强网络对关键特征的识别能力。

![image](https://cdn.nlark.com/yuque/__latex/6455d3f5235d607e9cd9c7ca4c558aa5.svg)

   例如，对一个 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 区域进行最大池化：

![image](https://cdn.nlark.com/yuque/__latex/63368eaaccf1d974e01d74a9d5016138.svg)

2. **平均池化（Average Pooling）**
    - 对局部区域中的所有值取平均值进行池化。
    - **作用**：平滑特征图，减小极端值的影响，常用于对特征图的平滑处理。

![image](https://cdn.nlark.com/yuque/__latex/0c4a9a856138df063c36273c5ab365e5.svg)

   例如，对一个 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 区域进行平均池化：

![image](https://cdn.nlark.com/yuque/__latex/cd8dc4611575742ffbbdeac5d33b6d23.svg)

3. **全局池化（Global Pooling）**
    - 对整个特征图进行池化，通常用于网络的最后几层。
    - 如 **全局最大池化** 和 **全局平均池化**，可以将整个特征图压缩成一个单一的值。

![image](https://cdn.nlark.com/yuque/__latex/dea742c543cbeabf91bd46e4645b3ba4.svg)

![image](https://cdn.nlark.com/yuque/__latex/06c9e472213684f7afff2e45e207b3ec.svg)

---

### **面试回答**
> **池化**主要通过减少特征图的空间维度，**降低计算量**，**增强鲁棒性**，**防止过拟合**，并且帮助提高**平移不变性**。常见的池化方式有最大池化和平均池化，最大池化保留区域内的最大特征，而平均池化则计算区域内的平均值。
>

---

### **如果面试官追问「池化和卷积的区别」**
> **卷积**是特征提取的操作，能够保留局部信息并通过学习卷积核来提取不同层次的特征；而**池化**是降维操作，主要用于减少计算量、降低特征图维度，并且帮助增强特征的鲁棒性。池化通常跟在卷积层之后。
>

---

## 什么是平移不变性
### 问题分析
**平移不变性（Translation Invariance）** 是指一个模型或算法对输入的平移（位置偏移）不敏感，即模型能够识别出物体的特征或模式，而不受物体在图像中的位置变化的影响。

在计算机视觉中，**平移不变性**意味着如果图像中的对象发生了平移（例如，物体从左边移动到右边），神经网络仍然能够正确地识别出该对象，而不需要重新学习位置相关的特征。

---

#### 为什么平移不变性重要？
+ **自然界的物体并不会总是出现在同一个位置**。例如，图像中的猫可能出现在左侧、中间或右侧，但我们希望模型能够始终识别出猫，而不受其位置变化的影响。
+ **提高模型的泛化能力**，让它能处理不同位置的物体，并且使得模型对位置信息的依赖较小。

---

#### 平移不变性如何通过池化和卷积实现？
1. **卷积操作**：
    - 卷积神经网络（CNN）通过使用共享的卷积核（滤波器）来提取图像特征。卷积核在整个图像上滑动，并对每个区域进行相同的操作，从而使网络能够在图像中识别特定的特征（例如边缘、纹理等），无论这些特征出现的位置如何。
    - 卷积操作天然具有**局部平移不变性**。即使图像中的物体发生了平移，卷积层仍然能够检测到相同的特征。
2. **池化操作**：
    - 池化操作，尤其是 **最大池化（Max Pooling）**，通过在特定的局部区域内选取最大值或平均值来降低特征图的分辨率。
    - 这一过程帮助提高平移不变性，因为它减小了微小的空间偏移（例如，物体的轻微平移）对特征表示的影响。
    - 例如，在进行最大池化时，如果图像的一个特征区域发生了平移，但这个区域中的最大值保持不变，那么池化层依然能够识别这个特征。

---

#### 平移不变性的示意
假设有一张猫的图片，无论猫出现在左上角、中间，还是右下角，卷积神经网络（CNN）通过其卷积和池化层，能够有效地识别猫的特征。这是因为卷积操作会提取局部特征（例如，耳朵、眼睛、轮廓等），而池化操作则帮助减少位置信息的影响，确保模型对平移不敏感。

---

### **面试回答**
> **平移不变性**是指模型能够识别出输入特征或物体，而不受物体在图像中的位置变化的影响。在卷积神经网络（CNN）中，卷积层和池化层自然具有平移不变性，因为卷积操作提取局部特征，池化操作则通过降维来减少位移对特征的影响。
>

---

## BERT 模型
### **问题分析**
> BERT，全称 **Bidirectional Encoder Representations from Transformers**，  
由 Google 2018 年提出，是一种基于 Transformer 编码器的**预训练语言模型**。
>

**主要特点有 3 点：**

1. **双向编码**：相比传统语言模型只从左到右，BERT 使用**Masked Language Model (MLM)**，可以同时利用左右上下文理解词语含义。
2. **预训练+微调**框架：BERT 在大规模语料（如 Wikipedia + BookCorpus）上预训练后，可以迁移到具体任务（如分类、问答）进行微调，效果优异。
3. **输入表示**：BERT 输入不仅有词向量，还有**Segment Embedding** 和 **Position Embedding**，可同时处理单句或句子对任务。

**典型应用：**文本分类、命名实体识别（NER）、阅读理解、句子匹配等。

---

### **面试回答**
「BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 编码器的预训练语言模型，特点是**双向上下文建模**，可以更好理解句子语义。BERT 先用**大规模文本预训练**，再通过**下游任务微调**，广泛应用于文本分类、问答等任务。」

### **加分答法**
> BERT 通过 Masked Language Model 和 Next Sentence Prediction 两个预训练目标，捕捉了词级别和句子级别的关系。  
后续如 RoBERTa、ALBERT、DistilBERT 都在 BERT 基础上进一步优化模型规模、效率和性能。
>
> 总之，BERT 通过双向预训练和灵活微调，显著推动了 NLP 任务效果，属于预训练语言模型的里程碑工作。
>

---

### BERT训练任务
「BERT 是通过自监督学习进行训练的，核心的训练任务有两个：**掩蔽语言模型 (MLM)** 和 **下一句预测 (NSP)**。

+ 在 MLM 中，随机掩蔽输入句子中的一些词，模型通过上下文预测这些被掩蔽的词。
+ 在 NSP 中，模型判断两个句子是否是连续的，从而学习句子级别的关系。  
BERT 的训练数据通过大规模语料库生成，并根据这些任务构建掩蔽和句子对，进行无监督的预训练。」

## Transformer
### **问题分析**
> Transformer 是 Google 2017 年提出的模型架构，特点是不依赖循环（RNN），完全基于**注意力机制**，同时并行计算效率高，广泛应用于自然语言处理和计算机视觉。
>

Transformer 主要分为**编码器 (Encoder)** 和**解码器 (Decoder)** 两部分，整体由多个堆叠的子层组成。

#### **Encoder 结构**
每个 Encoder Layer 包含：

1. **多头自注意力机制 (Multi-Head Self Attention)**
2. **前向全连接层 (Feed-Forward Network, FFN)**
3. **残差连接 + 层归一化 (Residual + LayerNorm)**

#### **Decoder 结构**
每个 Decoder Layer 多了一个模块，包含：

1. **Masked 多头自注意力机制 (Masked Multi-Head Self Attention)**
2. **Encoder-Decoder Attention**（用于关注 Encoder 输出）
3. **前向全连接层 (FFN)**
4. **残差连接 + 层归一化**

#### **核心原理 — 注意力机制 (Attention)**
注意力机制本质上通过**Query-Key-Value** 计算序列内部各个词之间的关系权重：

```plain
Attention(Q, K, V) = softmax(QKᵀ / √d_k) * V
```

这使得模型可以动态关注输入序列中与当前词最相关的信息。

#### **多头机制 (Multi-Head)**
将注意力机制并行分成多个子空间（多个头），增强模型捕捉多粒度特征的能力，最后拼接融合。

#### **位置编码 (Positional Encoding)**
因为 Transformer 不像 RNN 有顺序性，位置编码 (Positional Encoding) 用于引入序列位置信息，通常采用正余弦函数。

---

### **面试回答**
Transformer 通过堆叠的注意力机制和前向网络，结合残差与归一化，能高效捕捉长距离依赖，广泛用于 NLP 和 CV 任务中。Transformer 后续启发了 BERT（只用 Encoder）、GPT（只用 Decoder）、Vision Transformer (ViT) 等模型，成为现代 AI 基础架构之一。

---

## Transformer为什么可以并行
### 问题分析
---

#### 摒弃 RNN 中的串行依赖
+ RNN/LSTM 必须按时间步一个一个处理序列，后一个位置依赖前一个位置的隐藏状态，**无法并行**。
+ Transformer 完全去除了这种时间步依赖，**所有位置可以同时进行计算**。

---

#### 自注意力机制是“全局并行”的核心
+ 在 Transformer 中，每一个 token 的表示通过 **对整句中的所有 token 进行加权求和（Self-Attention）** 得到。
+ 这些注意力权重的计算可以通过矩阵操作一次性完成。

📌 举例：

> 输入序列长度为 n，那么 Self-Attention 的计算是通过 Q dot K^T 实现 n  的权重矩阵，这可以使用矩阵乘法一次性完成，天然适合 GPU 并行。
>

---

#### 位置编码用于补充顺序信息
+ Transformer 没有顺序结构，因此使用位置编码（如正余弦位置编码或 RoPE）提供序列顺序信息。
+ 这样既保持了并行计算，又保留了序列建模能力。

---

#### 全部采用矩阵操作，适合 GPU/TPU 加速
+ Transformer 中的注意力机制、前馈网络（FFN）、LayerNorm 等模块都是矩阵操作。
+ 矩阵乘法是 **现代硬件并行加速的最优场景**（比如 CUDA 核心可以大规模并发执行）。

---

### 面试回答
> Transformer 之所以能够很好地并行，是因为它摒弃了循环结构，采用了自注意力机制（Self-Attention），可以让每个位置同时看到全局信息，因此训练时可以用矩阵操作进行完全并行，非常适合 GPU 加速，可以同时处理整个序列中所有位置的输入，从而大大提升了训练和推理的并行效率。
>

---

## RNN
### 问题分析
> RNN（Recurrent Neural Network）是一种**循环神经网络**，专门用于处理**序列数据**，特点是通过隐藏状态 (Hidden State) 捕捉序列中的时序关系。
>

---

#### RNN 组成部分作用表
| 模块 | 作用 |
| --- | --- |
| **输入层 (Input)** | 输入当前时刻单词/数据的向量表示 |
| **隐藏状态 (Hidden State)** | **记忆上一个时刻的信息**，并与当前输入结合，捕捉序列依赖关系 |
| **权重矩阵 (Weights)** | 控制输入和隐藏状态如何影响当前状态；模型参数 |
| **输出层 (Output)** | 输出当前时刻预测结果 (如分类概率、生成词分布) |


**核心计算公式 (重点可以顺口说)**

```plain
hₜ = f(Wₕ * hₜ₋₁ + Wₓ * xₜ + b)  
```

> 当前隐藏状态 hₜ 由**前一个隐藏状态 hₜ₋₁ + 当前输入 xₜ**共同决定  
f 通常是 **tanh** 或 **ReLU** 激活函数
>

---

#### RNN 作用口诀式总结
> **一句话**：RNN 通过隐藏状态 h 让模型有「记忆」，捕捉**时间依赖关系**
>

**每个时间步做 3 件事：**  
1️⃣ 读当前输入  
2️⃣ 结合上一步隐藏状态  
3️⃣ 输出当前结果 & 更新隐藏状态

---

### **面试回答**
RNN（Recurrent Neural Network）是一种**循环神经网络**，专门用于处理**序列数据**，特点是通过隐藏状态 (Hidden State) 捕捉序列中的时序关系。优点是能处理序列数据，有「记忆能力」，缺点是计算不能并行，长序列易梯度消失/爆炸

---

「RNN 通过隐藏状态递归传递信息，捕捉序列依赖性，但存在长程依赖困难和效率低的问题，后续如 LSTM 和 Transformer 逐步解决了这些不足。」


「RNN 逐步处理序列，不能并行，捕捉长距离依赖困难；Transformer 用 Attention 机制全局建模关系，支持并行，效果更强。」

---

## LSTM和GRU
### 问题分析
LSTM（长短期记忆网络）和 GRU（门控循环单元）是对传统 RNN 的优化，它们通过引入**门控机制**来解决 RNN 在处理长序列时出现的**梯度消失**和**梯度爆炸**问题，从而更有效地学习长期依赖关系。面试时，你可以从这两个模型的结构、设计动机和相对于传统 RNN 的改进来回答。

---

### **1. LSTM（长短期记忆网络）**
LSTM 是由 Sepp Hochreiter 和 Jürgen Schmidhuber 于 1997 年提出的。它的核心思想是引入了**记忆单元**和三个**门控机制**，使得信息在网络中可以有效地保存或忘记，避免了 RNN 在长序列中训练时出现的梯度消失问题。

#### **LSTM 的主要组成部分**：
+ **记忆单元（Cell State）**：作为信息流的主要载体，记忆单元存储了长期的信息。信息可以在整个序列中沿着记忆单元传递。
+ **三个门**：
    1. **遗忘门 (Forget Gate)**：决定保留前一时刻的记忆多少。通过 sigmoid 函数决定遗忘多少信息，值为 0 时完全忘记，值为 1 时完全保留。
    2. **输入门 (Input Gate)**：决定当前输入 ![image](https://cdn.nlark.com/yuque/__latex/21c4616d966dca0cdc4d982b04f94933.svg) 对记忆单元的更新量。它通过 sigmoid 函数控制哪些信息应该被更新，以及通过 tanh 函数生成新的候选记忆。
    3. **输出门 (Output Gate)**：决定当前记忆单元的输出量。通过 sigmoid 函数控制哪些信息可以输出，结合当前记忆单元的状态，决定最终的输出。

#### **LSTM 的优势**：
+ **防止梯度消失**：LSTM 通过记忆单元中的“长期路径”减少了梯度消失问题，使得梯度能够更容易地流动。
+ **更强的长期依赖学习能力**：它能够存储和调整长期信息，使得 LSTM 能够处理更长的序列数据。

### **LSTM 的状态更新公式**：
![image](https://cdn.nlark.com/yuque/__latex/4d1398e4e9790cbe491702f4843ea2de.svg)

![image](https://cdn.nlark.com/yuque/__latex/a6fa5b57ae7cea75298ef6e6a5e41b52.svg)

![image](https://cdn.nlark.com/yuque/__latex/d0653b3eb1163a658a499b2e5d6026ab.svg)

![image](https://cdn.nlark.com/yuque/__latex/6d0880ef1c9b3505a4257acf0a79ec98.svg)

![image](https://cdn.nlark.com/yuque/__latex/9efc103419dd733627335fc2856b1f1f.svg)

![image](https://cdn.nlark.com/yuque/__latex/c6a5f65a01cd955a76aa5eb5c8222431.svg)

---

### **2. GRU（门控循环单元）**
GRU 是 Cho 等人于 2014 年提出的，是对 LSTM 的简化版本，旨在减少模型的复杂度，同时仍能有效处理长期依赖问题。GRU 在结构上只有两个门，分别是**更新门**和**重置门**，相比 LSTM 减少了计算量和参数。

#### **GRU 的主要组成部分**：
+ **更新门 (Update Gate)**：决定了前一时刻的隐藏状态 ![image](https://cdn.nlark.com/yuque/__latex/fe8efef06f21db580f77c6f95bdcfdc9.svg) 与当前时刻的候选隐藏状态 ![image](https://cdn.nlark.com/yuque/__latex/cb44739842a0bf855cac5f3e09833156.svg) 的比例，即控制了记忆的更新量。
+ **重置门 (Reset Gate)**：决定了前一时刻的隐藏状态 ![image](https://cdn.nlark.com/yuque/__latex/fe8efef06f21db580f77c6f95bdcfdc9.svg) 对当前时刻输入的影响，控制遗忘的比例。

#### **GRU 的优势**：
+ **简化了模型**：相较于 LSTM，GRU 没有记忆单元和输出门，计算量和参数较少，但仍能有效地捕捉长距离的依赖关系。
+ **更快的训练速度**：由于模型结构更简单，GRU 通常比 LSTM 更容易训练，尤其是在数据量较小或计算资源有限的情况下。

### **GRU 的状态更新公式**：
![image](https://cdn.nlark.com/yuque/__latex/26ae282a99d1586d8047a06cb5ff865f.svg)

![image](https://cdn.nlark.com/yuque/__latex/c8fa720e8a54f904834a2b67166d4f66.svg)

![image](https://cdn.nlark.com/yuque/__latex/e03e0353f47f64dccefc2794519c6f4a.svg)

![image](https://cdn.nlark.com/yuque/__latex/49074efc7e60b6d064e7738fe26952fc.svg)

---

### **LSTM 和 GRU 相比于 RNN 的优化**
+ **梯度消失问题**：RNN 在训练过程中，尤其是处理长序列时，会遇到梯度消失问题。LSTM 和 GRU 都通过引入门控机制，有效地保留了重要信息，并控制了信息的遗忘，缓解了梯度消失问题。
+ **长期依赖学习**：LSTM 通过其复杂的门控机制（如遗忘门和输入门）能够更好地捕捉长期依赖，而 GRU 用其简单的更新门和重置门也能有效地捕捉长期依赖。
+ **计算效率**：LSTM 模型较复杂，参数较多，训练时间较长；而 GRU 的结构较简单，计算效率较高。

---

### **面试回答**
「LSTM 和 GRU 是对传统 RNN 的优化，它们主要通过引入门控机制来解决 RNN 在长序列学习中的梯度消失问题。

+ LSTM 通过**遗忘门**、**输入门**和**输出门**来控制信息的传递和更新，从而有效捕捉长期依赖。
+ GRU 则通过**更新门**和**重置门**简化了模型结构，减少了计算量，尽管如此，它依然能够有效地捕捉长距离依赖。

这两种模型相比于传统 RNN，能够更有效地学习和记忆长期依赖，并且提高了训练效率，尤其是在处理长序列数据时表现更为出色。」

---

## 什么是embedding
### 问题分析
> Embedding 是一种将离散的、非连续的数据（如词、类别）映射为连续、低维稠密向量的方式，使模型能够更好地理解和处理这些数据。
>

---

#### 1️⃣ 为什么需要 Embedding？
+ 机器学习模型只能处理数值型数据。
+ 文本中的词语、图像中的类别标签本质上是离散符号（如“猫”、“人”、“跑”）。
+ 我们不能直接用 "one-hot" 编码：维度高、稀疏且不能表达语义关系。

---

#### 2️⃣ Embedding 是怎么做的？
+ 用一个**可学习的查找表（矩阵）**，将每个词或类别映射为一个低维稠密向量。
+ 假设词表大小为 `V`，embedding 维度为 `d`，那么 embedding 矩阵是 `V × d`。
+ 每个词的 embedding 是这个矩阵中对应行的向量。

---

#### 3️⃣ Embedding 的优势：
| 特性 | 描述 |
| --- | --- |
| **稠密低维** | 节省内存，提高计算效率 |
| **可学习** | 在训练中可以自动调整向量，使相似词靠近 |
| **保留语义关系** | “king - man + woman ≈ queen” 是 Word2Vec 的经典示例 |
| **支持迁移学习** | 可以用预训练 embedding，如 Word2Vec、GloVe、BERT embedding |


---

#### 4️⃣ 常见应用场景：
+ 自然语言处理：词嵌入（word embedding）、句子嵌入、段落嵌入
+ 推荐系统：用户 ID、商品 ID 的 embedding
+ 图神经网络：节点 embedding

---

### 面试回答
总的来说，Embedding 是一种将离散变量转化为模型可以理解的连续向量的技术，既提高了表达能力，又为神经网络提供了更强的泛化能力，是深度学习中非常核心的一项技术。

