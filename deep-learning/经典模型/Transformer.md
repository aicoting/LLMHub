## Transformer
### **é—®é¢˜åˆ†æž**
Transformer æ˜¯ Google 2017 å¹´æå‡ºçš„æ¨¡åž‹æž¶æž„ï¼Œç‰¹ç‚¹æ˜¯ä¸ä¾èµ–å¾ªçŽ¯ï¼ˆRNNï¼‰ï¼Œå®Œå…¨åŸºäºŽ**æ³¨æ„åŠ›æœºåˆ¶**ï¼ŒåŒæ—¶å¹¶è¡Œè®¡ç®—æ•ˆçŽ‡é«˜ï¼Œå¹¿æ³›åº”ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚

Transformer ä¸»è¦åˆ†ä¸º**ç¼–ç å™¨ (Encoder)** å’Œ**è§£ç å™¨ (Decoder)** ä¸¤éƒ¨åˆ†ï¼Œæ•´ä½“ç”±å¤šä¸ªå †å çš„å­å±‚ç»„æˆã€‚

#### **Encoder ç»“æž„**
æ¯ä¸ª Encoder Layer åŒ…å«ï¼š

1. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Self Attention)**
2. **å‰å‘å…¨è¿žæŽ¥å±‚ (Feed-Forward Network, FFN)**
3. **æ®‹å·®è¿žæŽ¥ + å±‚å½’ä¸€åŒ– (Residual + LayerNorm)**

#### **Decoder ç»“æž„**
æ¯ä¸ª Decoder Layer å¤šäº†ä¸€ä¸ªæ¨¡å—ï¼ŒåŒ…å«ï¼š

1. **Masked å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Masked Multi-Head Self Attention)**
2. **Encoder-Decoder Attention**ï¼ˆç”¨äºŽå…³æ³¨ Encoder è¾“å‡ºï¼‰
3. **å‰å‘å…¨è¿žæŽ¥å±‚ (FFN)**
4. **æ®‹å·®è¿žæŽ¥ + å±‚å½’ä¸€åŒ–**

#### **æ ¸å¿ƒåŽŸç† â€” æ³¨æ„åŠ›æœºåˆ¶ (Attention)**
æ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šé€šè¿‡**Query-Key-Value** è®¡ç®—åºåˆ—å†…éƒ¨å„ä¸ªè¯ä¹‹é—´çš„å…³ç³»æƒé‡ï¼š

```plain
Attention(Q, K, V) = softmax(QKáµ€ / âˆšd_k) * V
```

è¿™ä½¿å¾—æ¨¡åž‹å¯ä»¥åŠ¨æ€å…³æ³¨è¾“å…¥åºåˆ—ä¸­ä¸Žå½“å‰è¯æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

#### **å¤šå¤´æœºåˆ¶ (Multi-Head)**
å°†æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œåˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼ˆå¤šä¸ªå¤´ï¼‰ï¼Œå¢žå¼ºæ¨¡åž‹æ•æ‰å¤šç²’åº¦ç‰¹å¾çš„èƒ½åŠ›ï¼Œæœ€åŽæ‹¼æŽ¥èžåˆã€‚

#### **ä½ç½®ç¼–ç  (Positional Encoding)**
å› ä¸º Transformer ä¸åƒ RNN æœ‰é¡ºåºæ€§ï¼Œä½ç½®ç¼–ç  (Positional Encoding) ç”¨äºŽå¼•å…¥åºåˆ—ä½ç½®ä¿¡æ¯ï¼Œé€šå¸¸é‡‡ç”¨æ­£ä½™å¼¦å‡½æ•°ã€‚

### **é¢è¯•å›žç­”**
> Transformer é€šè¿‡å †å çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå‰å‘ç½‘ç»œï¼Œç»“åˆæ®‹å·®ä¸Žå½’ä¸€åŒ–ï¼Œèƒ½é«˜æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œå¹¿æ³›ç”¨äºŽ NLP å’Œ CV ä»»åŠ¡ä¸­ã€‚Transformer åŽç»­å¯å‘äº† BERTï¼ˆåªç”¨ Encoderï¼‰ã€GPTï¼ˆåªç”¨ Decoderï¼‰ã€Vision Transformer (ViT) ç­‰æ¨¡åž‹ï¼Œæˆä¸ºçŽ°ä»£ AI åŸºç¡€æž¶æž„ä¹‹ä¸€ã€‚
>

## Transformerä¸ºä»€ä¹ˆå¯ä»¥å¹¶è¡Œ
### é—®é¢˜åˆ†æž
#### æ‘’å¼ƒ RNN ä¸­çš„ä¸²è¡Œä¾èµ–
+ RNN/LSTM å¿…é¡»æŒ‰æ—¶é—´æ­¥ä¸€ä¸ªä¸€ä¸ªå¤„ç†åºåˆ—ï¼ŒåŽä¸€ä¸ªä½ç½®ä¾èµ–å‰ä¸€ä¸ªä½ç½®çš„éšè—çŠ¶æ€ï¼Œ**æ— æ³•å¹¶è¡Œ**ã€‚
+ Transformer å®Œå…¨åŽ»é™¤äº†è¿™ç§æ—¶é—´æ­¥ä¾èµ–ï¼Œ**æ‰€æœ‰ä½ç½®å¯ä»¥åŒæ—¶è¿›è¡Œè®¡ç®—**ã€‚

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯â€œå…¨å±€å¹¶è¡Œâ€çš„æ ¸å¿ƒ
+ åœ¨ Transformer ä¸­ï¼Œæ¯ä¸€ä¸ª token çš„è¡¨ç¤ºé€šè¿‡ **å¯¹æ•´å¥ä¸­çš„æ‰€æœ‰ token è¿›è¡ŒåŠ æƒæ±‚å’Œï¼ˆSelf-Attentionï¼‰** å¾—åˆ°ã€‚
+ è¿™äº›æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—å¯ä»¥é€šè¿‡çŸ©é˜µæ“ä½œä¸€æ¬¡æ€§å®Œæˆã€‚

ðŸ“Œ ä¸¾ä¾‹ï¼š

> è¾“å…¥åºåˆ—é•¿åº¦ä¸º nï¼Œé‚£ä¹ˆ Self-Attention çš„è®¡ç®—æ˜¯é€šè¿‡ Q dot K^T å®žçŽ° n  çš„æƒé‡çŸ©é˜µï¼Œè¿™å¯ä»¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•ä¸€æ¬¡æ€§å®Œæˆï¼Œå¤©ç„¶é€‚åˆ GPU å¹¶è¡Œã€‚
>

#### ä½ç½®ç¼–ç ç”¨äºŽè¡¥å……é¡ºåºä¿¡æ¯
+ Transformer æ²¡æœ‰é¡ºåºç»“æž„ï¼Œå› æ­¤ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£ä½™å¼¦ä½ç½®ç¼–ç æˆ– RoPEï¼‰æä¾›åºåˆ—é¡ºåºä¿¡æ¯ã€‚
+ è¿™æ ·æ—¢ä¿æŒäº†å¹¶è¡Œè®¡ç®—ï¼Œåˆä¿ç•™äº†åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚

#### å…¨éƒ¨é‡‡ç”¨çŸ©é˜µæ“ä½œï¼Œé€‚åˆ GPU/TPU åŠ é€Ÿ
+ Transformer ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ã€å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ã€LayerNorm ç­‰æ¨¡å—éƒ½æ˜¯çŸ©é˜µæ“ä½œã€‚
+ çŸ©é˜µä¹˜æ³•æ˜¯ **çŽ°ä»£ç¡¬ä»¶å¹¶è¡ŒåŠ é€Ÿçš„æœ€ä¼˜åœºæ™¯**ï¼ˆæ¯”å¦‚ CUDA æ ¸å¿ƒå¯ä»¥å¤§è§„æ¨¡å¹¶å‘æ‰§è¡Œï¼‰ã€‚

### é¢è¯•å›žç­”
> Transformer ä¹‹æ‰€ä»¥èƒ½å¤Ÿå¾ˆå¥½åœ°å¹¶è¡Œï¼Œæ˜¯å› ä¸ºå®ƒæ‘’å¼ƒäº†å¾ªçŽ¯ç»“æž„ï¼Œé‡‡ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå¯ä»¥è®©æ¯ä¸ªä½ç½®åŒæ—¶çœ‹åˆ°å…¨å±€ä¿¡æ¯ï¼Œå› æ­¤è®­ç»ƒæ—¶å¯ä»¥ç”¨çŸ©é˜µæ“ä½œè¿›è¡Œå®Œå…¨å¹¶è¡Œï¼Œéžå¸¸é€‚åˆ GPU åŠ é€Ÿï¼Œå¯ä»¥åŒæ—¶å¤„ç†æ•´ä¸ªåºåˆ—ä¸­æ‰€æœ‰ä½ç½®çš„è¾“å…¥ï¼Œä»Žè€Œå¤§å¤§æå‡äº†è®­ç»ƒå’ŒæŽ¨ç†çš„å¹¶è¡Œæ•ˆçŽ‡ã€‚
>

