# 基础知识点
## 梯度消失和梯度爆炸的原因是什么  
### 问题分析
梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）本质上都是在深层神经网络中反向传播过程中，梯度在多层传播时逐渐缩小或放大的问题，导致模型难以有效训练。

#### **1. 梯度消失的原因**
+ **链式法则累乘导致梯度趋近于0**  
在反向传播时，梯度是各层导数的连乘。如果激活函数（如sigmoid或tanh）的导数小于1，连续相乘后梯度迅速趋近0，导致前面层几乎不更新（学习停滞）。
+ **激活函数饱和**  
比如sigmoid在输入过大或过小时梯度接近0，进一步加剧梯度消失。
+ **权重初始化不合理**  
初始化权重过小也会导致信号逐步缩小。

#### **2. 梯度爆炸的原因**
+ **链式法则累乘导致梯度发散**  
如果激活函数的导数或权重大于1，连乘后梯度指数级增大，导致模型权重更新剧烈，甚至数值溢出。
+ **权重初始化过大**  
初始权重分布过大放大了信号。

### **面试回答**
> 梯度消失和梯度爆炸都是因为反向传播过程中梯度在多层传播时不断连乘，如果连乘的导数小于1就导致梯度消失，大于1则导致梯度爆炸。典型原因包括激活函数饱和（比如sigmoid）、权重初始化不合理和网络过深。
>

## 解决梯度消失和梯度爆炸的方法
### 问题分析
**“梯度消失和梯度爆炸是神经网络训练中常见的问题，主要出现在深层网络或RNN中。针对这两个问题，通常从以下几个方面入手进行处理：”**

#### 1. **权重初始化**
+ 使用合适的初始化方式可以缓解梯度问题：
    - 对于ReLU：使用 **He初始化**；
    - 对于sigmoid或tanh：使用 **Xavier初始化**。

#### 2. **使用合适的激活函数**
+ 避免使用容易饱和的激活函数（如 sigmoid）；
+ 优先使用 **ReLU** 及其变种（如 LeakyReLU, ELU），能有效缓解梯度消失。

#### 3. **使用归一化技术**
+ **Batch Normalization**：能使每一层的输入保持稳定，减缓梯度爆炸或消失；
+ **LayerNorm**（尤其适用于RNN、Transformer结构）。

#### 4. **梯度裁剪（Gradient Clipping）**
+ **主要用于RNN、LSTM**，在梯度过大时将其限制在某一范围内（如[-5, 5]）；

#### 5. **合理设置网络结构**
+ 减少不必要的层数；
+ 使用残差连接（Residual Connection），如 ResNet，有效缓解梯度消失。

#### 6. **优化器的选择**
+ 像 **Adam、RMSProp** 等自适应优化器，对梯度问题有一定缓解作用；

### 面试回答
> 面对梯度问题，可以从初始化、激活函数、归一化、结构设计等多方面综合考虑，确保网络训练稳定高效。
>

## 残差连接如何解决梯度爆炸和梯度消失
### **问题分析**
1. **残差连接是什么？**  
残差连接是在神经网络中引入的一种跳跃式连接方式，其公式为：

![image](https://cdn.nlark.com/yuque/__latex/ad468cbdf005efb70b3060f37bc505c9.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/a3803c9b72a55f8139079eb36aa883da.svg) 是要学习的残差函数（通常是若干个卷积层、ReLU等操作），![image](https://cdn.nlark.com/yuque/__latex/2adf48e1cf80858f0604d2b51eeed4b8.svg) 是输入，直接加到输出上。

2. **梯度消失/爆炸的本质原因**  
在传统深层神经网络中，梯度反向传播时需要连续乘多个梯度项（链式法则），如果每层的梯度值都小于 1（或大于 1），多次乘积会导致：
+ 梯度越来越小 → **梯度消失**
+ 梯度越来越大 → **梯度爆炸**
3. **残差连接如何缓解？**
+ 反向传播过程中，残差连接引入的恒等映射提供了**直接路径**，使得梯度可以“绕过”复杂的非线性变换，直接传播到更浅层，避免了长路径上的梯度连乘效应。

数学上，反向传播梯度可以表示为：

![image](https://cdn.nlark.com/yuque/__latex/3b30ed078fa983a19c63198030ca8ba1.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/df53f70d20a6a901dfd6da9f6986b470.svg) 是单位矩阵，表示恒等映射。  
即使 ![image](https://cdn.nlark.com/yuque/__latex/fb2e520265a45dd53d8aa560e0b96813.svg) 很小或很大，**“+I”** 保证了梯度至少可以稳定地传递下去。

4. **实际效果**
+ 使得网络更容易训练更深（比如 ResNet 可以训练 152 层以上的网络）
+ 缓解梯度消失，网络不会因为深度增加而性能下降（**退化问题 degradation problem**）

### **面试回答**
> 残差连接（Residual Connection）通过引入**恒等映射（identity mapping）**，使梯度在反向传播时可以直接沿跳跃路径（shortcut）传播，从而缓解了梯度消失和梯度爆炸问题，特别是在深层网络中，残差结构可以确保梯度不会因为连续非线性变换而过度缩小或放大，从而使深层网络更容易优化和训练。
>

## 归一化的作用
### **问题分析**
1. **梯度消失/爆炸产生的本质原因**
+ 神经网络每一层的输出经过非线性函数和权重变换，如果输入分布不稳定（方差过大或过小），经过多层之后，激活值和梯度可能迅速缩小或扩大 → 造成梯度消失或爆炸。
+ 特别在深层网络中，这种问题更严重。
2. **归一化怎么做的？**  
以 **Batch Normalization** 为例，对每一层的特征做：

![image](https://cdn.nlark.com/yuque/__latex/271c3576cd94fee8a3c312f44a650190.svg)

其中，![image](https://cdn.nlark.com/yuque/__latex/756a643380ff53c0692dbc2e7e930a35.svg) 和 ![image](https://cdn.nlark.com/yuque/__latex/217d731c58118430ccbb4f9f6d44ce08.svg) 是 mini-batch 中特征的均值和方差。

→ 结果：让激活值分布均值为 0，方差为 1。  
→ 激活值不会因为输入变化剧烈而偏离合理范围。

3. **为什么归一化可以缓解梯度消失/爆炸？**
+ **激活值保持稳定**：归一化限制了每一层输出的分布范围，不会快速放大/缩小，避免了激活值爆炸或接近 0 区域（比如 Sigmoid 饱和区）
+ **梯度保持稳定**：反向传播时，梯度与激活值直接相关，激活值稳定 → 梯度不会快速衰减或爆炸
+ **降低内部协变量偏移（Internal Covariate Shift）**：每层输入分布稳定，训练更稳定，学习率可以设得更大，加速收敛

### **面试回答**
> 归一化（Normalization）通过把每层输入或特征缩放到均值为 0、方差为 1 的分布，使得网络中的激活值保持在合理范围，从而防止梯度在前向传播或反向传播过程中因为数值过大或过小导致梯度爆炸或梯度消失。此外，归一化让每层的输入分布更稳定，有助于加速收敛并稳定训练。
>

## 为什么sigmoid会饱和  
### 问题分析
#### 标准定义
**Sigmoid 激活函数公式：**  
σ(x) = 1 / (1 + e^(-x))  
输出范围：（0, 1）

#### 为什么 sigmoid 会饱和？
1. **输入绝对值过大时梯度趋近于0**  
当输入 x 很大（远大于0）或很小（远小于0）时：
    - x → +∞，σ(x) → 1
    - x → -∞，σ(x) → 0
2. **导数几乎为0**  
Sigmoid 的导数公式为：  
σ'(x) = σ(x) * (1 - σ(x))  
当 σ(x) 接近 0 或 1 时，σ'(x) ≈ 0
3. **梯度消失的直接来源**  
反向传播时，梯度乘上这么小的导数，信息几乎无法向前层传递，称为「饱和区」。

### 面试简洁版回答模板
> Sigmoid 在输入绝对值很大时输出接近0或1，导数接近0。导数梯度几乎不传递，这叫做饱和，最终引发梯度消失问题。
>

## BatchNorm和LayerNorm有什么区别  
### 问题分析
| **维度** | **BatchNorm (BN)** | **LayerNorm (LN)** |
| --- | --- | --- |
| **归一化维度** | **样本间归一化**（batch维度） | **样本内归一化**（feature维度） |
| **计算公式** | 对每个特征维度 kkk 计算   μ和σ来自**batch中所有样本** | 对每个样本独立计算   μ和σ来自**特征维度** |
| **依赖 Batch Size** | **依赖 batch size**（小 batch 时效果差） | **不依赖 batch size** |
| **应用场景** | CNN、MLP 中常用 | Transformer、RNN 中常用 |
| **公式示例** | ![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1747120558875-3c870317-8fda-475f-8ece-dd7c4937bf80.png) | ![](https://cdn.nlark.com/yuque/0/2025/png/28454971/1747120568287-867c030b-3b85-4d68-89b0-8d2233db2ffd.png) |
| **运行时统计（推理阶段）** | 需要保存 moving mean/var | 不需要额外统计 |


### **面试回答**
> BatchNorm 对 batch 维度归一化，依赖多个样本，适合 CNN，但对小 batch 敏感；LayerNorm 对单个样本特征归一化，不依赖 batch size，更适合 RNN 和 Transformer 等序列模型。
>

## dropout的作用是什么  
### **问题分析**
**Dropout 是一种正则化方法，主要作用是防止过拟合 (overfitting)。**

#### **核心原理**
+ 训练时，**以一定概率随机丢弃神经元**（即设为0），让模型不能过度依赖某些特征
+ 这样每次训练都相当于在不同的「子网络」上更新参数  
→ **相当于集成多个模型 (model ensemble)**  
→ 提升模型的泛化能力

#### **数学表达**
训练时：

x~=x⋅m,m∼Bernoulli(p)

p为保留概率 (keep probability)

推理时：

+ 不丢节点，输出乘以 ppp 的缩放补偿（保持期望一致）

### **面试回答**
> Dropout 通过训练时随机丢弃神经元，防止模型过度依赖局部特征，相当于集成多个子网络，主要作用是降低过拟合、提升泛化能力。
>

### **如果面试官追问「为什么 Dropout 能防止过拟合？」**
> 因为 Dropout 迫使网络学到冗余且更鲁棒的特征表达，不依赖单一神经元，同时相当于集成多个子模型，增强模型稳定性。
>

## 什么是神经元和激活函数（**为什么要引入激活函数**）
### **面试回答**
> 神经元是神经网络的基本计算单元，主要作用是对输入特征加权整合并引入非线性，从而帮助模型提取和组合复杂特征。在不同网络中，神经元形式不同：MLP 中是全连接，CNN 中是局部感受野，RNN 中有时间依赖，Transformer 结合自注意力建模全局关系。
>
> 没有激活函数，神经网络只是线性模型，无法拟合复杂非线性问题；激活函数使模型具有非线性表达能力，从而增强模型拟合能力。
>

## 池化的作用是什么  
### 问题分析
池化（**Pooling**）是卷积神经网络（CNN）中的一种重要操作，主要用于减少空间尺寸、控制计算量、提高模型的鲁棒性，且常用于 **卷积层** 和 **全连接层** 之间。其核心作用是通过汇聚特征图中的信息，减少参数数量和计算复杂度，并增强模型的泛化能力。

#### 池化的主要作用
1. **降低计算复杂度**
    - 池化操作通过减小特征图的空间维度（例如，从 ![image](https://cdn.nlark.com/yuque/__latex/a11ea46280669ac0bf8afdac4019d77f.svg) 降到 ![image](https://cdn.nlark.com/yuque/__latex/5c8213948b3250fd57bd3a27a4232c6a.svg)），**减少了后续层的计算量**。
    - 这样可以减少内存消耗并加速训练过程。
2. **控制过拟合**
    - 池化层通过降低特征图的分辨率，**使得模型更具鲁棒性**，降低了对局部特征的依赖，间接地对模型起到了正则化作用，从而减少了过拟合的风险。
3. **保留重要特征**
    - 池化操作能够聚焦图像中的重要信息，并丢弃较不重要的部分，使得卷积网络更加专注于图像的关键信息。
    - 常见的池化操作（如最大池化）通过选取局部区域内的最大值来保留重要特征。
4. **增强模型的平移不变性**
    - 池化可以让网络对输入图像的平移产生较小的敏感度，提升了**平移不变性**。这意味着网络对图像中的小的平移变换不再过于敏感，能够更好地处理图像中位置的变化。

#### 常见池化类型
1. **最大池化（Max Pooling）**
    - 对局部区域（通常为 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 或 ![image](https://cdn.nlark.com/yuque/__latex/b08d861763b277c3ff6c7f92bc62b06c.svg)）中的最大值进行池化。
    - **作用**：保留该区域的最显著特征，增强网络对关键特征的识别能力。

![image](https://cdn.nlark.com/yuque/__latex/6455d3f5235d607e9cd9c7ca4c558aa5.svg)

   例如，对一个 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 区域进行最大池化：

![image](https://cdn.nlark.com/yuque/__latex/63368eaaccf1d974e01d74a9d5016138.svg)

2. **平均池化（Average Pooling）**
    - 对局部区域中的所有值取平均值进行池化。
    - **作用**：平滑特征图，减小极端值的影响，常用于对特征图的平滑处理。

![image](https://cdn.nlark.com/yuque/__latex/0c4a9a856138df063c36273c5ab365e5.svg)

   例如，对一个 ![image](https://cdn.nlark.com/yuque/__latex/829db47d500df094775efa8c62bdd9ea.svg) 区域进行平均池化：

![image](https://cdn.nlark.com/yuque/__latex/cd8dc4611575742ffbbdeac5d33b6d23.svg)

3. **全局池化（Global Pooling）**
    - 对整个特征图进行池化，通常用于网络的最后几层。
    - 如 **全局最大池化** 和 **全局平均池化**，可以将整个特征图压缩成一个单一的值。

![image](https://cdn.nlark.com/yuque/__latex/dea742c543cbeabf91bd46e4645b3ba4.svg)

![image](https://cdn.nlark.com/yuque/__latex/06c9e472213684f7afff2e45e207b3ec.svg)

### **面试回答**
> **池化**主要通过减少特征图的空间维度，**降低计算量**，**增强鲁棒性**，**防止过拟合**，并且帮助提高**平移不变性**。常见的池化方式有最大池化和平均池化，最大池化保留区域内的最大特征，而平均池化则计算区域内的平均值。
>

### **如果面试官追问「池化和卷积的区别」**
> **卷积**是特征提取的操作，能够保留局部信息并通过学习卷积核来提取不同层次的特征；而**池化**是降维操作，主要用于减少计算量、降低特征图维度，并且帮助增强特征的鲁棒性。池化通常跟在卷积层之后。
>



## 什么是平移不变性
### 问题分析
**平移不变性（Translation Invariance）** 是指一个模型或算法对输入的平移（位置偏移）不敏感，即模型能够识别出物体的特征或模式，而不受物体在图像中的位置变化的影响。

在计算机视觉中，**平移不变性**意味着如果图像中的对象发生了平移（例如，物体从左边移动到右边），神经网络仍然能够正确地识别出该对象，而不需要重新学习位置相关的特征。

#### 为什么平移不变性重要？
+ **自然界的物体并不会总是出现在同一个位置**。例如，图像中的猫可能出现在左侧、中间或右侧，但我们希望模型能够始终识别出猫，而不受其位置变化的影响。
+ **提高模型的泛化能力**，让它能处理不同位置的物体，并且使得模型对位置信息的依赖较小。

#### 平移不变性如何通过池化和卷积实现？
1. **卷积操作**：
    - 卷积神经网络（CNN）通过使用共享的卷积核（滤波器）来提取图像特征。卷积核在整个图像上滑动，并对每个区域进行相同的操作，从而使网络能够在图像中识别特定的特征（例如边缘、纹理等），无论这些特征出现的位置如何。
    - 卷积操作天然具有**局部平移不变性**。即使图像中的物体发生了平移，卷积层仍然能够检测到相同的特征。
2. **池化操作**：
    - 池化操作，尤其是 **最大池化（Max Pooling）**，通过在特定的局部区域内选取最大值或平均值来降低特征图的分辨率。
    - 这一过程帮助提高平移不变性，因为它减小了微小的空间偏移（例如，物体的轻微平移）对特征表示的影响。
    - 例如，在进行最大池化时，如果图像的一个特征区域发生了平移，但这个区域中的最大值保持不变，那么池化层依然能够识别这个特征。

#### 平移不变性的示意
假设有一张猫的图片，无论猫出现在左上角、中间，还是右下角，卷积神经网络（CNN）通过其卷积和池化层，能够有效地识别猫的特征。这是因为卷积操作会提取局部特征（例如，耳朵、眼睛、轮廓等），而池化操作则帮助减少位置信息的影响，确保模型对平移不敏感。

### **面试回答**
> **平移不变性**是指模型能够识别出输入特征或物体，而不受物体在图像中的位置变化的影响。在卷积神经网络（CNN）中，卷积层和池化层自然具有平移不变性，因为卷积操作提取局部特征，池化操作则通过降维来减少位移对特征的影响。
>

## 什么是embedding
### 问题分析
Embedding 是一种将离散的、非连续的数据（如词、类别）映射为连续、低维稠密向量的方式，使模型能够更好地理解和处理这些数据。

#### 1️⃣ 为什么需要 Embedding？
+ 机器学习模型只能处理数值型数据。
+ 文本中的词语、图像中的类别标签本质上是离散符号（如“猫”、“人”、“跑”）。
+ 我们不能直接用 "one-hot" 编码：维度高、稀疏且不能表达语义关系。

#### 2️⃣ Embedding 是怎么做的？
+ 用一个**可学习的查找表（矩阵）**，将每个词或类别映射为一个低维稠密向量。
+ 假设词表大小为 `V`，embedding 维度为 `d`，那么 embedding 矩阵是 `V × d`。
+ 每个词的 embedding 是这个矩阵中对应行的向量。

#### 3️⃣ Embedding 的优势：
| 特性 | 描述 |
| --- | --- |
| **稠密低维** | 节省内存，提高计算效率 |
| **可学习** | 在训练中可以自动调整向量，使相似词靠近 |
| **保留语义关系** | “king - man + woman ≈ queen” 是 Word2Vec 的经典示例 |
| **支持迁移学习** | 可以用预训练 embedding，如 Word2Vec、GloVe、BERT embedding |


#### 4️⃣ 常见应用场景：
+ 自然语言处理：词嵌入（word embedding）、句子嵌入、段落嵌入
+ 推荐系统：用户 ID、商品 ID 的 embedding
+ 图神经网络：节点 embedding

### 面试回答
> Embedding 是一种将离散变量转化为模型可以理解的连续向量的技术，既提高了表达能力，又为神经网络提供了更强的泛化能力，是深度学习中非常核心的一项技术。
>



